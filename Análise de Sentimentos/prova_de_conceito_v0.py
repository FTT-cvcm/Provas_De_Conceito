# -*- coding: utf-8 -*-
"""Prova de conceito - V0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v-BDq6FG-Kcbnk672p6WxK9PYGJwdIAm

Elaborado com o Notebook Python Google Colab
"""

# Load the Drive helper and mount
from google.colab import drive

# This will prompt for authorization.
drive.mount('/content/drive')

!ls "/content/drive/My Drive/Databases"

import pandas as pd

dados = pd.read_csv("/content/drive/My Drive/Databases/NoThemeTweets.csv")

dados

classificacao = dados["sentiment"].replace(["Negativo","Positivo"],[0,1])

dados["classificacao"] = classificacao

dados.groupby('classificacao').count()

dados

import nltk
from nltk import tokenize

nltk.download('stopwords')

token_espaco = tokenize.WhitespaceTokenizer()

token_pontuacao = tokenize.WordPunctTokenizer()

palavras_irrelevantes = nltk.corpus.stopwords.words("portuguese")

from string import punctuation

pontuacao = list()
for ponto in punctuation:
  pontuacao.append(ponto)

excluir = [":)",":(","://","https"]

pontuacao_stopwords = pontuacao + palavras_irrelevantes + excluir

frase_processada = list()
for tweet in dados.tweet_text:
  nova_frase = list()
  tweet = tweet.lower()
  palavras_texto = token_espaco.tokenize(tweet)
  for palavra in palavras_texto:
    if '@' not in palavra:
      if palavra not in pontuacao_stopwords:      
        nova_frase.append(palavra)
  frase_processada.append(' '.join(nova_frase))

dados["tratamento_1"] = frase_processada

dados.head()

!pip install unidecode

import unidecode

sem_acentos = [unidecode.unidecode(tweet) for tweet in dados["tratamento_1"]]

dados["tratamento_2"] = sem_acentos

dados.head()

nltk.download('rslp')

stopwords_sem_acento = [unidecode.unidecode(texto) for texto in pontuacao_stopwords]

stemer = nltk.RSLPStemmer()

frase_processada = list()
for tweet in dados["tratamento_2"]:
  nova_frase = list()
  palavras_texto = token_pontuacao.tokenize(tweet)
  for palavra in palavras_texto:
    if palavra not in stopwords_sem_acento:
      nova_frase.append(stemer.stem(palavra))
  frase_processada.append(' '.join(nova_frase))

dados["tratamento_3"] = frase_processada

dados

import matplotlib.pyplot as plt
from wordcloud import WordCloud
import seaborn as sns

def nuvem_palavras_neg(texto, coluna_texto):  
  texto_negativo = texto.query("sentiment == 'Negativo'")
  todas_palavras = ' '.join([texto for texto in texto_negativo[coluna_texto]])

  nuvem_palavras = WordCloud(width=800, height=500, max_font_size=110, collocations=False).generate(todas_palavras)

  plt.figure(figsize=(10,7))
  plt.imshow(nuvem_palavras, interpolation='bilinear')
  plt.axis("off")
  plt.show()

def nuvem_palavras_pos(texto, coluna_texto):  
  texto_positivo = texto.query("sentiment == 'Positivo'")
  todas_palavras = ' '.join([texto for texto in texto_positivo[coluna_texto]])

  nuvem_palavras = WordCloud(width=800, height=500, max_font_size=110, collocations=False).generate(todas_palavras)

  plt.figure(figsize=(10,7))
  plt.imshow(nuvem_palavras, interpolation='bilinear')
  plt.axis("off")
  plt.show()

def pareto(texto, coluna_texto, quantidade):
  todas_palavras = ' '.join([texto for texto in texto[coluna_texto]])

  token_frase = token_espaco.tokenize(todas_palavras)
  frequencia = nltk.FreqDist(token_frase)
  df_frequencia = pd.DataFrame({"Palavra":list(frequencia.keys()),"Frequência":list(frequencia.values())})
  df_frequencia = df_frequencia.nlargest(quantidade, "Frequência")
  plt.figure(figsize=(12,8))
  ax = sns.barplot(data= df_frequencia, x = "Palavra", y = "Frequência")
  ax.set(ylabel= "Contagem")
  plt.show()

nuvem_palavras_neg(dados, "tratamento_3")

nuvem_palavras_pos(dados, "tratamento_3")

pareto(dados, "tratamento_3", 10)

def trata_frase(frase):
  frase_tratada = list()
  nova_frase = list()
  frase = frase.lower()
  palavras_texto = token_espaco.tokenize(frase)
  for palavra in palavras_texto:
    if '@' not in palavra:
      if palavra not in pontuacao_stopwords:      
        nova_frase.append(stemer.stem(palavra))
  frase_tratada.append(' '.join(nova_frase))
  frase_tratada = [unidecode.unidecode(frase) for frase in frase_tratada]
  
  return frase_tratada

from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import train_test_split

tfidf1 = TfidfVectorizer(lowercase = False, max_features=50)

tfidf_bruto = tfidf1.fit_transform(dados["tweet_text"])

treino, teste, classe_treino, classe_teste = train_test_split(tfidf_bruto, dados["classificacao"], random_state=42)

regressao_logistica1 = LogisticRegression(solver = "lbfgs")

regressao_logistica1.fit(treino, classe_treino)

acuracia_tfidf_bruto = regressao_logistica1.score(teste, classe_teste)

print(acuracia_tfidf_bruto)

tfidf2 = TfidfVectorizer(lowercase = False, max_features=50)

tfidf_tratado = tfidf2.fit_transform(dados["tratamento_3"])

treino, teste, classe_treino, classe_teste = train_test_split(tfidf_tratado, dados["classificacao"], random_state=42)

regressao_logistica2 = LogisticRegression(solver = "lbfgs")

regressao_logistica2.fit(treino, classe_treino)

acuracia_tfidf_tratado = regressao_logistica2.score(teste, classe_teste)

print(acuracia_tfidf_tratado)

tfidf3 = TfidfVectorizer(lowercase=False,ngram_range=(1,2))

tfidf_ngrams = tfidf3.fit_transform(dados["tratamento_3"])
treino, teste, classe_treino, classe_teste = train_test_split(tfidf_ngrams, dados["classificacao"], random_state=42)
regressao_logistica3 = LogisticRegression(solver = "lbfgs", max_iter=1000)
regressao_logistica3.fit(treino, classe_treino)
acuracia_tfidf_ngrams = regressao_logistica3.score(teste, classe_teste)

print(acuracia_tfidf_ngrams)

print(tfidf1.get_feature_names())

trata_frase("Faltá Frase")

def testa_frase(frase):
  print("Regressão 1")
  print(regressao_logistica1.predict(tfidf1.transform(trata_frase(frase))))
  print(regressao_logistica1.predict_proba(tfidf1.transform(trata_frase(frase))))
  print("Regressão 2")
  print(regressao_logistica2.predict(tfidf2.transform(trata_frase(frase))))
  print(regressao_logistica2.predict_proba(tfidf2.transform(trata_frase(frase))))
  print("Regressão 3")
  print(regressao_logistica3.predict(tfidf3.transform(trata_frase(frase))))
  print(regressao_logistica3.predict_proba(tfidf3.transform(trata_frase(frase))))

testa_frase("Um dia ruim")
testa_frase("Um dia bom")

"""Resultados Encontrados

O treinamento sem o tratamento dos dados gerou uma taxa de acerto de 0.6881, já com o tratamento dos dados a taxa vai para 0.6885, o que não representa um ganho significativo. Utilizando-se um bigram a taxa de acerto foi de 0.7949, que representa um aumento de 15,5% com relação aos dados sem tratamento.
Foi possível observar também que a confiança do resultado é significativamente maior utilizando bigrams.
"""